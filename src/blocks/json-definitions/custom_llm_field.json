{"type": "AI21", "template": "custom_llm_field", "settings": [], "properties": [{"name": "type", "description": "Is the type of <b>llm</b> to be applied while <i>configuring the llm</i>", "type": "select", "options": [{"name": "AI21"}, {"name": "AlephAlpha"}, {"name": "Anthropic"}, {"name": "Anyscale"}, {"name": "AzureOpenAI"}, {"name": "Banana"}, {"name": "Beam"}, {"name": "Bedrock"}, {"name": "CTransformers"}, {"name": "CerebriumAI"}, {"name": "Cohere"}, {"name": "Databricks"}, {"name": "DeepInfra"}, {"name": "FakeListLLM"}, {"name": "ForefrontAI"}, {"name": "GPT4All"}, {"name": "GooglePalm"}, {"name": "GooseAI"}, {"name": "HuggingFaceEndpoint"}, {"name": "HuggingFaceHub"}, {"name": "HuggingFacePipeline"}, {"name": "HuggingFaceTextGenInference"}, {"name": "HumanInputLLM"}, {"name": "LlamaCpp"}, {"name": "Modal"}, {"name": "MosaicML"}, {"name": "NLPCloud"}, {"name": "OpenAI"}, {"name": "OpenAIChat"}, {"name": "OpenLM"}, {"name": "Petals"}, {"name": "PipelineAI"}, {"name": "PredictionGuard"}, {"name": "PromptLayerOpenAI"}, {"name": "PromptLayerOpenAIChat"}, {"name": "RWKV"}, {"name": "Replicate"}, {"name": "SagemakerEndpoint"}, {"name": "SelfHostedHuggingFaceLLM"}, {"name": "SelfHostedPipeline"}, {"name": "StochasticAI"}, {"name": "VertexAI"}, {"name": "Writer"}], "optionName": "name", "default": 0, "refProperty": "settings", "required": true}, {"name": "template", "description": "Select the <b>template</b> source", "type": "select", "options": [], "optionName": "name", "optionTitle": "category", "default": 0, "hideIfSingleOption": true, "required": true}, {"name": "settings", "type": "array", "required": true, "refProperty": "type", "itemProperties": [{"name": "AI21", "properties": [{"name": "verbose", "description": "Is the value to the AI21 llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model", "description": "Is the value to the AI21 llm <b>model</b> argument", "type": "input", "default": "j2-jumbo-instruct", "required": true}, {"name": "temperature", "description": "Is the value to the AI21 llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.7, "required": true}, {"name": "maxTokens", "description": "Is the value to the AI21 llm <b>maxTokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "minTokens", "description": "Is the value to the AI21 llm <b>minTokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 0, "required": true}, {"name": "topP", "description": "Is the value to the AI21 llm <b>topP</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}]}, {"name": "AlephAlpha", "properties": [{"name": "verbose", "description": "Is the value to the AlephAlpha llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "maximum_tokens", "description": "Is the value to the AlephAlpha llm <b>maximum_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 64, "required": true}, {"name": "temperature", "description": "Is the value to the AlephAlpha llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "top_k", "description": "Is the value to the AlephAlpha llm <b>top_k</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 0, "required": true}, {"name": "top_p", "description": "Is the value to the AlephAlpha llm <b>top_p</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "presence_penalty", "description": "Is the value to the AlephAlpha llm <b>presence_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "frequency_penalty", "description": "Is the value to the AlephAlpha llm <b>frequency_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "n", "description": "Is the value to the AlephAlpha llm <b>n</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "echo", "description": "Is the value to the AlephAlpha llm <b>echo</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "use_multiplicative_frequency_penalty", "description": "Is the value to the AlephAlpha llm <b>use_multiplicative_frequency_penalty</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "sequence_penalty", "description": "Is the value to the AlephAlpha llm <b>sequence_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "sequence_penalty_min_length", "description": "Is the value to the AlephAlpha llm <b>sequence_penalty_min_length</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 2, "required": true}, {"name": "use_multiplicative_sequence_penalty", "description": "Is the value to the AlephAlpha llm <b>use_multiplicative_sequence_penalty</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "completion_bias_inclusion_first_token_only", "description": "Is the value to the AlephAlpha llm <b>completion_bias_inclusion_first_token_only</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "completion_bias_exclusion_first_token_only", "description": "Is the value to the AlephAlpha llm <b>completion_bias_exclusion_first_token_only</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "repetition_penalties_include_completion", "description": "Is the value to the AlephAlpha llm <b>repetition_penalties_include_completion</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "raw_completion", "description": "Is the value to the AlephAlpha llm <b>raw_completion</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "Anthropic", "properties": [{"name": "model", "description": "Is the value to the Anthropic llm <b>model</b> argument", "type": "input", "default": "claude-v1", "required": true}, {"name": "max_tokens_to_sample", "description": "Is the value to the Anthropic llm <b>max_tokens_to_sample</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "streaming", "description": "Is the value to the Anthropic llm <b>streaming</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "verbose", "description": "Is the value to the Anthropic llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}]}, {"name": "Anyscale", "properties": [{"name": "verbose", "description": "Is the value to the Anyscale llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}]}, {"name": "AzureOpenAI", "properties": [{"name": "verbose", "description": "Is the value to the AzureOpenAI llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model", "description": "Is the value to the AzureOpenAI llm <b>model</b> argument", "type": "input", "default": "text-davinci-003", "required": true}, {"name": "temperature", "description": "Is the value to the AzureOpenAI llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.7, "required": true}, {"name": "max_tokens", "description": "Is the value to the AzureOpenAI llm <b>max_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "top_p", "description": "Is the value to the AzureOpenAI llm <b>top_p</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}, {"name": "frequency_penalty", "description": "Is the value to the AzureOpenAI llm <b>frequency_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "presence_penalty", "description": "Is the value to the AzureOpenAI llm <b>presence_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "n", "description": "Is the value to the AzureOpenAI llm <b>n</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "best_of", "description": "Is the value to the AzureOpenAI llm <b>best_of</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "batch_size", "description": "Is the value to the AzureOpenAI llm <b>batch_size</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 20, "required": true}, {"name": "max_retries", "description": "Is the value to the AzureOpenAI llm <b>max_retries</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 6, "required": true}, {"name": "streaming", "description": "Is the value to the AzureOpenAI llm <b>streaming</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "Banana", "properties": [{"name": "verbose", "description": "Is the value to the Banana llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_key", "description": "Is the value to the Banana llm <b>model_key</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "Beam", "properties": [{"name": "verbose", "description": "Is the value to the Beam llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_name", "description": "Is the value to the Beam llm <b>model_name</b> argument", "type": "input", "default": "", "required": true}, {"name": "name", "description": "Is the value to the Beam llm <b>name</b> argument", "type": "input", "default": "", "required": true}, {"name": "cpu", "description": "Is the value to the Beam llm <b>cpu</b> argument", "type": "input", "default": "", "required": true}, {"name": "memory", "description": "Is the value to the Beam llm <b>memory</b> argument", "type": "input", "default": "", "required": true}, {"name": "gpu", "description": "Is the value to the Beam llm <b>gpu</b> argument", "type": "input", "default": "", "required": true}, {"name": "python_version", "description": "Is the value to the Beam llm <b>python_version</b> argument", "type": "input", "default": "", "required": true}, {"name": "max_length", "description": "Is the value to the Beam llm <b>max_length</b> argument", "type": "input", "default": "", "required": true}, {"name": "url", "description": "Is the value to the Beam llm <b>url</b> argument", "type": "input", "default": "", "required": true}, {"name": "beam_client_id", "description": "Is the value to the Beam llm <b>beam_client_id</b> argument", "type": "input", "default": "", "required": true}, {"name": "beam_client_secret", "description": "Is the value to the Beam llm <b>beam_client_secret</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "Bedrock", "properties": [{"name": "verbose", "description": "Is the value to the Bedrock llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_id", "description": "Is the value to the Bedrock llm <b>model_id</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "CTransformers", "properties": [{"name": "verbose", "description": "Is the value to the CTransformers llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model", "description": "Is the value to the CTransformers llm <b>model</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "CerebriumAI", "properties": [{"name": "verbose", "description": "Is the value to the CerebriumAI llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "endpoint_url", "description": "Is the value to the CerebriumAI llm <b>endpoint_url</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "Cohere", "properties": [{"name": "verbose", "description": "Is the value to the Cohere llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "max_tokens", "description": "Is the value to the Cohere llm <b>max_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "temperature", "description": "Is the value to the Cohere llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.75, "required": true}, {"name": "k", "description": "Is the value to the Cohere llm <b>k</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 0, "required": true}, {"name": "p", "description": "Is the value to the Cohere llm <b>p</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "frequency_penalty", "description": "Is the value to the Cohere llm <b>frequency_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "presence_penalty", "description": "Is the value to the Cohere llm <b>presence_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "max_retries", "description": "Is the value to the Cohere llm <b>max_retries</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 10, "required": true}]}, {"name": "Databricks", "properties": [{"name": "verbose", "description": "Is the value to the Databricks llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "host", "description": "Is the value to the Databricks llm <b>host</b> argument", "type": "input", "default": "None", "required": true}, {"name": "api_token", "description": "Is the value to the Databricks llm <b>api_token</b> argument", "type": "input", "default": "None", "required": true}]}, {"name": "DeepInfra", "properties": [{"name": "verbose", "description": "Is the value to the DeepInfra llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_id", "description": "Is the value to the DeepInfra llm <b>model_id</b> argument", "type": "input", "default": "google/flan-t5-xl", "required": true}]}, {"name": "FakeListLLM", "properties": [{"name": "verbose", "description": "Is the value to the FakeListLLM llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "i", "description": "Is the value to the FakeListLLM llm <b>i</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 0, "required": true}]}, {"name": "ForefrontAI", "properties": [{"name": "verbose", "description": "Is the value to the ForefrontAI llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "endpoint_url", "description": "Is the value to the ForefrontAI llm <b>endpoint_url</b> argument", "type": "input", "default": "", "required": true}, {"name": "temperature", "description": "Is the value to the ForefrontAI llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.7, "required": true}, {"name": "length", "description": "Is the value to the ForefrontAI llm <b>length</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "top_p", "description": "Is the value to the ForefrontAI llm <b>top_p</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}, {"name": "top_k", "description": "Is the value to the ForefrontAI llm <b>top_k</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 40, "required": true}, {"name": "repetition_penalty", "description": "Is the value to the ForefrontAI llm <b>repetition_penalty</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}]}, {"name": "GPT4All", "properties": [{"name": "verbose", "description": "Is the value to the GPT4All llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model", "description": "Is the value to the GPT4All llm <b>model</b> argument", "type": "input", "default": "", "required": true}, {"name": "n_ctx", "description": "Is the value to the GPT4All llm <b>n_ctx</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 512, "required": true}, {"name": "n_parts", "description": "Is the value to the GPT4All llm <b>n_parts</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": -1, "required": true}, {"name": "seed", "description": "Is the value to the GPT4All llm <b>seed</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 0, "required": true}, {"name": "f16_kv", "description": "Is the value to the GPT4All llm <b>f16_kv</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "logits_all", "description": "Is the value to the GPT4All llm <b>logits_all</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "vocab_only", "description": "Is the value to the GPT4All llm <b>vocab_only</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "use_mlock", "description": "Is the value to the GPT4All llm <b>use_mlock</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "embedding", "description": "Is the value to the GPT4All llm <b>embedding</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "n_batch", "description": "Is the value to the GPT4All llm <b>n_batch</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "streaming", "description": "Is the value to the GPT4All llm <b>streaming</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "context_erase", "description": "Is the value to the GPT4All llm <b>context_erase</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.5, "required": true}, {"name": "allow_download", "description": "Is the value to the GPT4All llm <b>allow_download</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "GooglePalm", "properties": [{"name": "verbose", "description": "Is the value to the GooglePalm llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_name", "description": "Is the value to the GooglePalm llm <b>model_name</b> argument", "type": "input", "default": "models/text-bison-001", "required": true}, {"name": "temperature", "description": "Is the value to the GooglePalm llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.7, "required": true}, {"name": "n", "description": "Is the value to the GooglePalm llm <b>n</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}]}, {"name": "GooseAI", "properties": [{"name": "verbose", "description": "Is the value to the GooseAI llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_name", "description": "Is the value to the GooseAI llm <b>model_name</b> argument", "type": "input", "default": "gpt-neo-20b", "required": true}, {"name": "temperature", "description": "Is the value to the GooseAI llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.7, "required": true}, {"name": "max_tokens", "description": "Is the value to the GooseAI llm <b>max_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "top_p", "description": "Is the value to the GooseAI llm <b>top_p</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}, {"name": "min_tokens", "description": "Is the value to the GooseAI llm <b>min_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "frequency_penalty", "description": "Is the value to the GooseAI llm <b>frequency_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "presence_penalty", "description": "Is the value to the GooseAI llm <b>presence_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "n", "description": "Is the value to the GooseAI llm <b>n</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}]}, {"name": "HuggingFaceEndpoint", "properties": [{"name": "verbose", "description": "Is the value to the HuggingFaceEndpoint llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "endpoint_url", "description": "Is the value to the HuggingFaceEndpoint llm <b>endpoint_url</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "HuggingFaceHub", "properties": [{"name": "verbose", "description": "Is the value to the HuggingFaceHub llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "repo_id", "description": "Is the value to the HuggingFaceHub llm <b>repo_id</b> argument", "type": "input", "default": "gpt2", "required": true}]}, {"name": "HuggingFacePipeline", "properties": [{"name": "verbose", "description": "Is the value to the HuggingFacePipeline llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_id", "description": "Is the value to the HuggingFacePipeline llm <b>model_id</b> argument", "type": "input", "default": "gpt2", "required": true}]}, {"name": "HuggingFaceTextGenInference", "properties": [{"name": "verbose", "description": "Is the value to the HuggingFaceTextGenInference llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "max_new_tokens", "description": "Is the value to the HuggingFaceTextGenInference llm <b>max_new_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 512, "required": true}, {"name": "temperature", "description": "Is the value to the HuggingFaceTextGenInference llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.8, "required": true}, {"name": "inference_server_url", "description": "Is the value to the HuggingFaceTextGenInference llm <b>inference_server_url</b> argument", "type": "input", "default": "", "required": true}, {"name": "timeout", "description": "Is the value to the HuggingFaceTextGenInference llm <b>timeout</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 120, "required": true}, {"name": "stream", "description": "Is the value to the HuggingFaceTextGenInference llm <b>stream</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "HumanInputLLM", "properties": [{"name": "verbose", "description": "Is the value to the HumanInputLLM llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "separator", "description": "Is the value to the HumanInputLLM llm <b>separator</b> argument", "type": "input", "default": "\\n", "required": true}]}, {"name": "LlamaCpp", "properties": [{"name": "verbose", "description": "Is the value to the LlamaCpp llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_path", "description": "Is the value to the LlamaCpp llm <b>model_path</b> argument", "type": "input", "default": "", "required": true}, {"name": "n_ctx", "description": "Is the value to the LlamaCpp llm <b>n_ctx</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 512, "required": true}, {"name": "n_parts", "description": "Is the value to the LlamaCpp llm <b>n_parts</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": -1, "required": true}, {"name": "seed", "description": "Is the value to the LlamaCpp llm <b>seed</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": -1, "required": true}, {"name": "f16_kv", "description": "Is the value to the LlamaCpp llm <b>f16_kv</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "logits_all", "description": "Is the value to the LlamaCpp llm <b>logits_all</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "vocab_only", "description": "Is the value to the LlamaCpp llm <b>vocab_only</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "use_mlock", "description": "Is the value to the LlamaCpp llm <b>use_mlock</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "streaming", "description": "Is the value to the LlamaCpp llm <b>streaming</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "Modal", "properties": [{"name": "verbose", "description": "Is the value to the Modal llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "endpoint_url", "description": "Is the value to the Modal llm <b>endpoint_url</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "MosaicML", "properties": [{"name": "verbose", "description": "Is the value to the MosaicML llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "endpoint_url", "description": "Is the value to the MosaicML llm <b>endpoint_url</b> argument", "type": "input", "default": "https://models.hosted-on.mosaicml.hosting/mpt-7b-instruct/v1/predict", "required": true}, {"name": "inject_instruction_format", "description": "Is the value to the MosaicML llm <b>inject_instruction_format</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "retry_sleep", "description": "Is the value to the MosaicML llm <b>retry_sleep</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}]}, {"name": "NLPCloud", "properties": [{"name": "verbose", "description": "Is the value to the NLPCloud llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_name", "description": "Is the value to the NLPCloud llm <b>model_name</b> argument", "type": "input", "default": "finetuned-gpt-neox-20b", "required": true}, {"name": "temperature", "description": "Is the value to the NLPCloud llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.7, "required": true}, {"name": "min_length", "description": "Is the value to the NLPCloud llm <b>min_length</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "max_length", "description": "Is the value to the NLPCloud llm <b>max_length</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "length_no_input", "description": "Is the value to the NLPCloud llm <b>length_no_input</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "remove_input", "description": "Is the value to the NLPCloud llm <b>remove_input</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "remove_end_sequence", "description": "Is the value to the NLPCloud llm <b>remove_end_sequence</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "top_p", "description": "Is the value to the NLPCloud llm <b>top_p</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "top_k", "description": "Is the value to the NLPCloud llm <b>top_k</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 50, "required": true}, {"name": "repetition_penalty", "description": "Is the value to the NLPCloud llm <b>repetition_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}, {"name": "length_penalty", "description": "Is the value to the NLPCloud llm <b>length_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}, {"name": "do_sample", "description": "Is the value to the NLPCloud llm <b>do_sample</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "num_beams", "description": "Is the value to the NLPCloud llm <b>num_beams</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "early_stopping", "description": "Is the value to the NLPCloud llm <b>early_stopping</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "num_return_sequences", "description": "Is the value to the NLPCloud llm <b>num_return_sequences</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}]}, {"name": "OpenAI", "properties": [{"name": "verbose", "description": "Is the value to the OpenAI llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model", "description": "Is the value to the OpenAI llm <b>model</b> argument", "type": "input", "default": "text-davinci-003", "required": true}, {"name": "temperature", "description": "Is the value to the OpenAI llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.7, "required": true}, {"name": "max_tokens", "description": "Is the value to the OpenAI llm <b>max_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "top_p", "description": "Is the value to the OpenAI llm <b>top_p</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}, {"name": "frequency_penalty", "description": "Is the value to the OpenAI llm <b>frequency_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "presence_penalty", "description": "Is the value to the OpenAI llm <b>presence_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "n", "description": "Is the value to the OpenAI llm <b>n</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "best_of", "description": "Is the value to the OpenAI llm <b>best_of</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "batch_size", "description": "Is the value to the OpenAI llm <b>batch_size</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 20, "required": true}, {"name": "max_retries", "description": "Is the value to the OpenAI llm <b>max_retries</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 6, "required": true}, {"name": "streaming", "description": "Is the value to the OpenAI llm <b>streaming</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "OpenAIChat", "properties": [{"name": "verbose", "description": "Is the value to the OpenAIChat llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_name", "description": "Is the value to the OpenAIChat llm <b>model_name</b> argument", "type": "input", "default": "gpt-3.5-turbo", "required": true}, {"name": "max_retries", "description": "Is the value to the OpenAIChat llm <b>max_retries</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 6, "required": true}, {"name": "streaming", "description": "Is the value to the OpenAIChat llm <b>streaming</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "OpenLM", "properties": [{"name": "verbose", "description": "Is the value to the OpenLM llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model", "description": "Is the value to the OpenLM llm <b>model</b> argument", "type": "input", "default": "text-davinci-003", "required": true}, {"name": "temperature", "description": "Is the value to the OpenLM llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.7, "required": true}, {"name": "max_tokens", "description": "Is the value to the OpenLM llm <b>max_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "top_p", "description": "Is the value to the OpenLM llm <b>top_p</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}, {"name": "frequency_penalty", "description": "Is the value to the OpenLM llm <b>frequency_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "presence_penalty", "description": "Is the value to the OpenLM llm <b>presence_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "n", "description": "Is the value to the OpenLM llm <b>n</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "best_of", "description": "Is the value to the OpenLM llm <b>best_of</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "batch_size", "description": "Is the value to the OpenLM llm <b>batch_size</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 20, "required": true}, {"name": "max_retries", "description": "Is the value to the OpenLM llm <b>max_retries</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 6, "required": true}, {"name": "streaming", "description": "Is the value to the OpenLM llm <b>streaming</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "Petals", "properties": [{"name": "verbose", "description": "Is the value to the Petals llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_name", "description": "Is the value to the Petals llm <b>model_name</b> argument", "type": "input", "default": "bigscience/bloom-petals", "required": true}, {"name": "temperature", "description": "Is the value to the Petals llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.7, "required": true}, {"name": "max_new_tokens", "description": "Is the value to the Petals llm <b>max_new_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "top_p", "description": "Is the value to the Petals llm <b>top_p</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.9, "required": true}, {"name": "do_sample", "description": "Is the value to the Petals llm <b>do_sample</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "PipelineAI", "properties": [{"name": "verbose", "description": "Is the value to the PipelineAI llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "pipeline_key", "description": "Is the value to the PipelineAI llm <b>pipeline_key</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "PredictionGuard", "properties": [{"name": "verbose", "description": "Is the value to the PredictionGuard llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "max_tokens", "description": "Is the value to the PredictionGuard llm <b>max_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "temperature", "description": "Is the value to the PredictionGuard llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.75, "required": true}]}, {"name": "PromptLayerOpenAI", "properties": [{"name": "verbose", "description": "Is the value to the PromptLayerOpenAI llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model", "description": "Is the value to the PromptLayerOpenAI llm <b>model</b> argument", "type": "input", "default": "text-davinci-003", "required": true}, {"name": "temperature", "description": "Is the value to the PromptLayerOpenAI llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.7, "required": true}, {"name": "max_tokens", "description": "Is the value to the PromptLayerOpenAI llm <b>max_tokens</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "top_p", "description": "Is the value to the PromptLayerOpenAI llm <b>top_p</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}, {"name": "frequency_penalty", "description": "Is the value to the PromptLayerOpenAI llm <b>frequency_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "presence_penalty", "description": "Is the value to the PromptLayerOpenAI llm <b>presence_penalty</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.0, "required": true}, {"name": "n", "description": "Is the value to the PromptLayerOpenAI llm <b>n</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "best_of", "description": "Is the value to the PromptLayerOpenAI llm <b>best_of</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 1, "required": true}, {"name": "batch_size", "description": "Is the value to the PromptLayerOpenAI llm <b>batch_size</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 20, "required": true}, {"name": "max_retries", "description": "Is the value to the PromptLayerOpenAI llm <b>max_retries</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 6, "required": true}, {"name": "streaming", "description": "Is the value to the PromptLayerOpenAI llm <b>streaming</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "PromptLayerOpenAIChat", "properties": [{"name": "verbose", "description": "Is the value to the PromptLayerOpenAIChat llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_name", "description": "Is the value to the PromptLayerOpenAIChat llm <b>model_name</b> argument", "type": "input", "default": "gpt-3.5-turbo", "required": true}, {"name": "max_retries", "description": "Is the value to the PromptLayerOpenAIChat llm <b>max_retries</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 6, "required": true}, {"name": "streaming", "description": "Is the value to the PromptLayerOpenAIChat llm <b>streaming</b> argument", "type": "checkbox", "default": true, "required": true}]}, {"name": "RWKV", "properties": [{"name": "verbose", "description": "Is the value to the RWKV llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model", "description": "Is the value to the RWKV llm <b>model</b> argument", "type": "input", "default": "", "required": true}, {"name": "tokens_path", "description": "Is the value to the RWKV llm <b>tokens_path</b> argument", "type": "input", "default": "", "required": true}, {"name": "strategy", "description": "Is the value to the RWKV llm <b>strategy</b> argument", "type": "input", "default": "cpu fp32", "required": true}, {"name": "rwkv_verbose", "description": "Is the value to the RWKV llm <b>rwkv_verbose</b> argument", "type": "checkbox", "default": true, "required": true}, {"name": "temperature", "description": "Is the value to the RWKV llm <b>temperature</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 1.0, "required": true}, {"name": "top_p", "description": "Is the value to the RWKV llm <b>top_p</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.5, "required": true}, {"name": "penalty_alpha_frequency", "description": "Is the value to the RWKV llm <b>penalty_alpha_frequency</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.4, "required": true}, {"name": "penalty_alpha_presence", "description": "Is the value to the RWKV llm <b>penalty_alpha_presence</b> argument", "type": "number", "min": 0.1, "max": 100.0, "step": 0.1, "default": 0.4, "required": true}, {"name": "CHUNK_LEN", "description": "Is the value to the RWKV llm <b>CHUNK_LEN</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}, {"name": "max_tokens_per_generation", "description": "Is the value to the RWKV llm <b>max_tokens_per_generation</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 256, "required": true}]}, {"name": "Replicate", "properties": [{"name": "verbose", "description": "Is the value to the Replicate llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model", "description": "Is the value to the Replicate llm <b>model</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "SagemakerEndpoint", "properties": [{"name": "verbose", "description": "Is the value to the SagemakerEndpoint llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "endpoint_name", "description": "Is the value to the SagemakerEndpoint llm <b>endpoint_name</b> argument", "type": "input", "default": "", "required": true}, {"name": "region_name", "description": "Is the value to the SagemakerEndpoint llm <b>region_name</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "SelfHostedHuggingFaceLLM", "properties": [{"name": "verbose", "description": "Is the value to the SelfHostedHuggingFaceLLM llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_id", "description": "Is the value to the SelfHostedHuggingFaceLLM llm <b>model_id</b> argument", "type": "input", "default": "gpt2", "required": true}, {"name": "task", "description": "Is the value to the SelfHostedHuggingFaceLLM llm <b>task</b> argument", "type": "input", "default": "text-generation", "required": true}, {"name": "device", "description": "Is the value to the SelfHostedHuggingFaceLLM llm <b>device</b> argument", "type": "number", "min": 1, "max": 100, "step": 1, "default": 0, "required": true}]}, {"name": "SelfHostedPipeline", "properties": [{"name": "verbose", "description": "Is the value to the SelfHostedPipeline llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}]}, {"name": "StochasticAI", "properties": [{"name": "verbose", "description": "Is the value to the StochasticAI llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "api_url", "description": "Is the value to the StochasticAI llm <b>api_url</b> argument", "type": "input", "default": "", "required": true}]}, {"name": "VertexAI", "properties": [{"name": "verbose", "description": "Is the value to the VertexAI llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}]}, {"name": "Writer", "properties": [{"name": "verbose", "description": "Is the value to the Writer llm <b>verbose</b> argument", "type": "checkbox", "default": false, "required": true}, {"name": "model_id", "description": "Is the value to the Writer llm <b>model_id</b> argument", "type": "input", "default": "palmyra-instruct", "required": true}, {"name": "logprobs", "description": "Is the value to the Writer llm <b>logprobs</b> argument", "type": "checkbox", "default": true, "required": true}]}]}]}